{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WideNet prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Loaded Parameters:\n",
      " 40000 35 0.2 200 \n",
      " dataset/glove/glove.twitter.27B.200d.txt\n",
      "[i] Importing Modules...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Using Keras version 2.1.4\n",
      "[i] Finished Importing Modules\n",
      "[i] . Reading from csv file...Done!\n",
      "[i] Found 30447 unique tokens.\n",
      "[+] Shape of data tensor: (40000, 30)\n",
      "[+] Shape of label tensor: (40000, 4)\n",
      "[+] Number of entries in each category:\n",
      "[+] Training:\n",
      " [ 7681. 10922. 12264.  1133.]\n",
      "[+] Validation:\n",
      " [1963. 2702. 3035.  300.]\n",
      "[i] Loading GloVe from: dataset/glove/glove.twitter.27B.200d.txt ...Done.\n",
      "[+] Proceeding with Embedding Matrix...Completed!\n",
      "Finished running setup.\n"
     ]
    }
   ],
   "source": [
    "%run Setup.ipynb\n",
    "%run ExtraFunctions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed the input vector into the embedding layer, then feed the resulting sequence into a bidirectional LSTM layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "l_lstm1 = Bidirectional(LSTM(4,dropout=0.2,recurrent_dropout=0.2,return_sequences=True))(embedded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel 7: best accuracy, prevent over fitting!\n",
    "l_conv_7 = Conv1D(filters=12,kernel_size=7,activation='relu',kernel_regularizer=regularizers.l2(0.02))(l_lstm1)\n",
    "# kernel 2: good accuracy\n",
    "l_conv_2 = Conv1D(filters=12,kernel_size=2,activation='relu',kernel_regularizer=regularizers.l2(0.01))(l_lstm1)\n",
    "# kernel 6: good accuracy\n",
    "l_conv_6 = Conv1D(filters=12,kernel_size=6,activation='relu',kernel_regularizer=regularizers.l2(0.01))(l_lstm1)\n",
    "# kernel 3: good accuracy, little over fitting\n",
    "l_conv_3 = Conv1D(filters=12,kernel_size=3,activation='relu')(l_lstm1)\n",
    "\n",
    "inception.append(l_conv_7)\n",
    "inception.append(l_conv_2)\n",
    "inception.append(l_conv_6)\n",
    "inception.append(l_conv_3)\n",
    "\n",
    "# poorer performing layers\n",
    "l_conv_4 = Conv1D(filters=6,kernel_size=4,activation='relu',kernel_regularizer=regularizers.l2(0.01))(l_lstm1)\n",
    "l_conv_5 = Conv1D(filters=6,kernel_size=5,activation='relu',kernel_regularizer=regularizers.l2(0.02))(l_lstm1)\n",
    "l_conv_8 = Conv1D(filters=6,kernel_size=8,activation='relu',kernel_regularizer=regularizers.l2(0.02))(l_lstm1)\n",
    "\n",
    "inception.append(l_conv_4)\n",
    "inception.append(l_conv_5)\n",
    "inception.append(l_conv_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last segment: max-pooling layer\n",
    "l_pool_i1 = MaxPooling1D(3)(l_lstm1)\n",
    "l_conv_1 = Conv1D(filters=8,kernel_size=1,\n",
    "                    activation='relu',kernel_regularizer=regularizers.l2(0.02))(l_pool_i1)\n",
    "inception.append(l_conv_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_merge = Concatenate(axis=1)(inception)\n",
    "l_pool1 = MaxPooling1D(3)(l_merge)\n",
    "l_drop1 = Dropout(0.4)(l_pool1)\n",
    "l_flat = Flatten()(l_drop1)\n",
    "l_dense = Dense(16, activation='relu',kernel_regularizer=regularizers.l2(0.01))(l_flat)\n",
    "preds = Dense(4, activation='softmax')(l_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(sequence_input, preds)\n",
    "adadelta = optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "lr_metric = get_lr_metric(adadelta)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adadelta,\n",
    "              metrics=['acc', lr_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=16, write_grads=True , write_graph=True)\n",
    "model_checkpoints = callbacks.ModelCheckpoint(\"checkpoint.h5\", monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "lr_schedule = callbacks.LearningRateScheduler(initial_boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "model.save('WideNet.h5')\n",
    "print(\"Training Progress:\")\n",
    "model_log = model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          epochs=200, batch_size=50,\n",
    "          callbacks=[tensorboard, lr_schedule])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "model.save('WideNet.h5')\n",
    "pd.DataFrame(model_log.history).to_csv(\"history-inception.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
