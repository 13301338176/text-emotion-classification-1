{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Text Classification with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Setup.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load embedding matrix into an `Embedding` layer. Toggle `trainable=False` to prevent the weights from being updated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deeper 1D CNN\n",
    "[Reference](https://github.com/richliao/textClassifier), [Source](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html) and [Notes](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/)\n",
    "\n",
    "Deeper Convolutional neural network: In [CNN for Sentence Classification](http://www.aclweb.org/anthology/D14-1181) (Yoon Kim, 2014), multiple filters have been applied. This can be implemented using Keras `Merge` Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "convs, filter_sizes = [], [1,3,5]\n",
    "for fsz in filter_sizes:\n",
    "    l_conv = Conv1D(filters=16,kernel_size=fsz,activation='relu',kernel_regularizer=regularizers.l2(0.01))(embedded_sequences)\n",
    "    l_pool = MaxPooling1D(2)(l_conv)\n",
    "    convs.append(l_pool)\n",
    "\n",
    "l_merge = Concatenate(axis=1)(convs)\n",
    "l_cov1= Conv1D(16, 3, activation='relu',kernel_regularizer=regularizers.l2(0.01))(l_merge)\n",
    "l_pool1 = MaxPooling1D(2)(l_cov1)\n",
    "l_drop1 = Dropout(0.1)(l_pool1)\n",
    "l_cov2 = Conv1D(16, 3, activation='relu',kernel_regularizer=regularizers.l2(0.01))(l_drop1)\n",
    "l_pool2 = MaxPooling1D(17)(l_cov2) # global max pooling\n",
    "l_flat = Flatten()(l_pool2)\n",
    "l_dense = Dense(16, activation='relu')(l_flat)\n",
    "preds = Dense(4, activation='softmax')(l_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Model(sequence_input, preds)\n",
    "adadelta = optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adadelta,\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def step_cyclic(epoch):\n",
    "    try:\n",
    "        l_r, decay = 1.0, 0.0001\n",
    "        if epoch%15==0:multiplier = 10\n",
    "        else:multiplier = 1\n",
    "        rate = float(multiplier * l_r * 1/(1 + decay * epoch))\n",
    "        print(\"learning_rate\",round(rate,4))\n",
    "        return rate\n",
    "    except Exception as e:\n",
    "        print(\"Error in lr_schedule:\",str(e))\n",
    "        return float(1.0)\n",
    "    \n",
    "def initial_boost(epoch):\n",
    "    if epoch==0: return float(6.0)\n",
    "    else: return float(1.0)\n",
    "        \n",
    "tensorboard = callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=256, write_grads=True , write_graph=True)\n",
    "model_checkpoints = callbacks.ModelCheckpoint(\"checkpoints\", monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "lr_schedule = callbacks.LearningRateScheduler(step_cyclic)\n",
    "#lr_schedule = callbacks.LearningRateScheduler(initial_boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_26 (InputLayer)           (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_17 (Embedding)        (None, 30, 200)      6108600     input_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_125 (Conv1D)             (None, 30, 16)       3216        embedding_17[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_126 (Conv1D)             (None, 28, 16)       9616        embedding_17[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_127 (Conv1D)             (None, 26, 16)       16016       embedding_17[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_125 (MaxPooling1D (None, 15, 16)       0           conv1d_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_126 (MaxPooling1D (None, 14, 16)       0           conv1d_126[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_127 (MaxPooling1D (None, 13, 16)       0           conv1d_127[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 42, 16)       0           max_pooling1d_125[0][0]          \n",
      "                                                                 max_pooling1d_126[0][0]          \n",
      "                                                                 max_pooling1d_127[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_128 (Conv1D)             (None, 40, 16)       784         concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_128 (MaxPooling1D (None, 20, 16)       0           conv1d_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 20, 16)       0           max_pooling1d_128[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_129 (Conv1D)             (None, 18, 16)       784         dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_129 (MaxPooling1D (None, 1, 16)        0           conv1d_129[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_22 (Flatten)            (None, 16)           0           max_pooling1d_129[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_43 (Dense)                (None, 16)           272         flatten_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_44 (Dense)                (None, 4)            68          dense_43[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 6,139,356\n",
      "Trainable params: 30,756\n",
      "Non-trainable params: 6,108,600\n",
      "__________________________________________________________________________________________________\n",
      "Training Progress:\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/200\n",
      "learning_rate 10.0\n",
      "32000/32000 [==============================] - 11s 343us/step - loss: 1.2120 - acc: 0.4744 - val_loss: 1.1294 - val_acc: 0.5317\n",
      "Epoch 2/200\n",
      "learning_rate 0.9999\n",
      "32000/32000 [==============================] - 11s 340us/step - loss: 1.0773 - acc: 0.5507 - val_loss: 1.0565 - val_acc: 0.5639\n",
      "Epoch 3/200\n",
      "learning_rate 0.9998\n",
      "32000/32000 [==============================] - 11s 340us/step - loss: 1.0535 - acc: 0.5597 - val_loss: 1.0477 - val_acc: 0.5583\n",
      "Epoch 4/200\n",
      "learning_rate 0.9997\n",
      "32000/32000 [==============================] - 11s 352us/step - loss: 1.0469 - acc: 0.5617 - val_loss: 1.0416 - val_acc: 0.5686\n",
      "Epoch 5/200\n",
      "learning_rate 0.9996\n",
      "32000/32000 [==============================] - 11s 359us/step - loss: 1.0439 - acc: 0.5646 - val_loss: 1.0408 - val_acc: 0.5679\n",
      "Epoch 6/200\n",
      "learning_rate 0.9995\n",
      "32000/32000 [==============================] - 11s 355us/step - loss: 1.0409 - acc: 0.5631 - val_loss: 1.0441 - val_acc: 0.5636\n",
      "Epoch 7/200\n",
      "learning_rate 0.9994\n",
      "32000/32000 [==============================] - 11s 349us/step - loss: 1.0397 - acc: 0.5663 - val_loss: 1.0372 - val_acc: 0.5720\n",
      "Epoch 8/200\n",
      "learning_rate 0.9993\n",
      "32000/32000 [==============================] - 11s 353us/step - loss: 1.0391 - acc: 0.5637 - val_loss: 1.0444 - val_acc: 0.5611\n",
      "Epoch 9/200\n",
      "learning_rate 0.9992\n",
      "32000/32000 [==============================] - 12s 359us/step - loss: 1.0369 - acc: 0.5668 - val_loss: 1.0420 - val_acc: 0.5644\n",
      "Epoch 10/200\n",
      "learning_rate 0.9991\n",
      "32000/32000 [==============================] - 11s 356us/step - loss: 1.0368 - acc: 0.5669 - val_loss: 1.0360 - val_acc: 0.5721\n",
      "Epoch 11/200\n",
      "learning_rate 0.999\n",
      "32000/32000 [==============================] - 11s 354us/step - loss: 1.0367 - acc: 0.5647 - val_loss: 1.0772 - val_acc: 0.5476\n",
      "Epoch 12/200\n",
      "learning_rate 0.9989\n",
      "32000/32000 [==============================] - 11s 345us/step - loss: 1.0364 - acc: 0.5673 - val_loss: 1.0395 - val_acc: 0.5695\n",
      "Epoch 13/200\n",
      "learning_rate 0.9988\n",
      "32000/32000 [==============================] - 11s 353us/step - loss: 1.0357 - acc: 0.5658 - val_loss: 1.0391 - val_acc: 0.5643\n",
      "Epoch 14/200\n",
      "learning_rate 0.9987\n",
      "32000/32000 [==============================] - 12s 361us/step - loss: 1.0347 - acc: 0.5653 - val_loss: 1.0384 - val_acc: 0.5675\n",
      "Epoch 15/200\n",
      "learning_rate 0.9986\n",
      "32000/32000 [==============================] - 11s 355us/step - loss: 1.0349 - acc: 0.5656 - val_loss: 1.0436 - val_acc: 0.5649\n",
      "Epoch 16/200\n",
      "learning_rate 9.985\n",
      "32000/32000 [==============================] - 11s 352us/step - loss: 1.1266 - acc: 0.5245 - val_loss: 1.1141 - val_acc: 0.5374\n",
      "Epoch 17/200\n",
      "learning_rate 0.9984\n",
      "32000/32000 [==============================] - 11s 345us/step - loss: 1.0648 - acc: 0.5571 - val_loss: 1.0489 - val_acc: 0.5640\n",
      "Epoch 18/200\n",
      "learning_rate 0.9983\n",
      "32000/32000 [==============================] - 11s 353us/step - loss: 1.0438 - acc: 0.5648 - val_loss: 1.0388 - val_acc: 0.5704\n",
      "Epoch 19/200\n",
      "learning_rate 0.9982\n",
      "32000/32000 [==============================] - 11s 352us/step - loss: 1.0388 - acc: 0.5638 - val_loss: 1.0658 - val_acc: 0.5516\n",
      "Epoch 20/200\n",
      "learning_rate 0.9981\n",
      "32000/32000 [==============================] - 11s 356us/step - loss: 1.0366 - acc: 0.5662 - val_loss: 1.0428 - val_acc: 0.5644\n",
      "Epoch 21/200\n",
      "learning_rate 0.998\n",
      "32000/32000 [==============================] - 12s 368us/step - loss: 1.0343 - acc: 0.5669 - val_loss: 1.0496 - val_acc: 0.5559\n",
      "Epoch 22/200\n",
      "learning_rate 0.9979\n",
      "32000/32000 [==============================] - 11s 346us/step - loss: 1.0342 - acc: 0.5676 - val_loss: 1.0492 - val_acc: 0.5609\n",
      "Epoch 23/200\n",
      "learning_rate 0.9978\n",
      "32000/32000 [==============================] - 11s 358us/step - loss: 1.0332 - acc: 0.5684 - val_loss: 1.0456 - val_acc: 0.5617\n",
      "Epoch 24/200\n",
      "learning_rate 0.9977\n",
      "32000/32000 [==============================] - 11s 355us/step - loss: 1.0350 - acc: 0.5647 - val_loss: 1.0426 - val_acc: 0.5656\n",
      "Epoch 25/200\n",
      "learning_rate 0.9976\n",
      "32000/32000 [==============================] - 11s 348us/step - loss: 1.0336 - acc: 0.5655 - val_loss: 1.0409 - val_acc: 0.5697\n",
      "Epoch 26/200\n",
      "learning_rate 0.9975\n",
      "32000/32000 [==============================] - 10s 324us/step - loss: 1.0326 - acc: 0.5665 - val_loss: 1.0363 - val_acc: 0.5690\n",
      "Epoch 27/200\n",
      "learning_rate 0.9974\n",
      "32000/32000 [==============================] - 10s 321us/step - loss: 1.0320 - acc: 0.5676 - val_loss: 1.0369 - val_acc: 0.5657\n",
      "Epoch 28/200\n",
      "learning_rate 0.9973\n",
      "32000/32000 [==============================] - 12s 365us/step - loss: 1.0323 - acc: 0.5648 - val_loss: 1.0405 - val_acc: 0.5624\n",
      "Epoch 29/200\n",
      "learning_rate 0.9972\n",
      "32000/32000 [==============================] - 11s 329us/step - loss: 1.0337 - acc: 0.5663 - val_loss: 1.0332 - val_acc: 0.5696\n",
      "Epoch 30/200\n",
      "learning_rate 0.9971\n",
      "32000/32000 [==============================] - 11s 330us/step - loss: 1.0328 - acc: 0.5665 - val_loss: 1.0359 - val_acc: 0.5707\n",
      "Epoch 31/200\n",
      "learning_rate 9.9701\n",
      "32000/32000 [==============================] - 11s 329us/step - loss: 1.1234 - acc: 0.5275 - val_loss: 1.1231 - val_acc: 0.5291\n",
      "Epoch 32/200\n",
      "learning_rate 0.9969\n",
      "32000/32000 [==============================] - 11s 337us/step - loss: 1.0642 - acc: 0.5566 - val_loss: 1.0534 - val_acc: 0.5649\n",
      "Epoch 33/200\n",
      "learning_rate 0.9968\n",
      "32000/32000 [==============================] - 11s 337us/step - loss: 1.0442 - acc: 0.5637 - val_loss: 1.0460 - val_acc: 0.5577\n",
      "Epoch 34/200\n",
      "learning_rate 0.9967\n",
      "32000/32000 [==============================] - 11s 349us/step - loss: 1.0382 - acc: 0.5659 - val_loss: 1.0380 - val_acc: 0.5679\n",
      "Epoch 35/200\n",
      "learning_rate 0.9966\n",
      "32000/32000 [==============================] - 11s 339us/step - loss: 1.0343 - acc: 0.5688 - val_loss: 1.0364 - val_acc: 0.5679\n",
      "Epoch 36/200\n",
      "learning_rate 0.9965\n",
      "32000/32000 [==============================] - 11s 351us/step - loss: 1.0344 - acc: 0.5678 - val_loss: 1.0370 - val_acc: 0.5691\n",
      "Epoch 37/200\n",
      "learning_rate 0.9964\n",
      "32000/32000 [==============================] - 11s 345us/step - loss: 1.0333 - acc: 0.5657 - val_loss: 1.0406 - val_acc: 0.5646\n",
      "Epoch 38/200\n",
      "learning_rate 0.9963\n",
      "32000/32000 [==============================] - 11s 348us/step - loss: 1.0325 - acc: 0.5668 - val_loss: 1.0462 - val_acc: 0.5590\n",
      "Epoch 39/200\n",
      "learning_rate 0.9962\n",
      "16224/32000 [==============>...............] - ETA: 5s - loss: 1.0260 - acc: 0.5715"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-b8bca1be14d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m model.fit(x_train, y_train, validation_data=(x_val, y_val),\n\u001b[1;32m      4\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m           callbacks=[tensorboard, model_checkpoints, lr_schedule])\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1710\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m         \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m   2475\u001b[0m                               **self.session_kwargs)\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mcandidate_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_initialized'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m                     \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "print(\"Training Progress:\")\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          epochs=200, batch_size=32,\n",
    "          callbacks=[tensorboard, model_checkpoints, lr_schedule])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
