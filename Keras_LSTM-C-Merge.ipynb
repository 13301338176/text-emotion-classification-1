{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM-CNN Text Classification with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Loaded Parameters:\n",
      " 40000 30 0.2 50 \n",
      " dataset/glove/glove.twitter.27B.50d.txt\n",
      "[i] Importing Modules...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Using Keras version 2.1.4\n",
      "[i] Finished Importing Modules\n",
      "[i] . Reading from csv file...Done!\n",
      "[i] Found 30692 unique tokens.\n",
      "[+] Shape of data tensor: (40000, 30)\n",
      "[+] Shape of label tensor: (40000, 4)\n",
      "[+] Number of entries in each category:\n",
      "[+] Training:\n",
      " [ 7708. 10913. 12238.  1141.]\n",
      "[+] Validation:\n",
      " [1936. 2711. 3061.  292.]\n",
      "[i] Loading GloVe from: dataset/glove/glove.twitter.27B.50d.txt ...Done.\n",
      "[+] Proceeding with Embedding Matrix...Completed!\n",
      "Finished running setup.\n"
     ]
    }
   ],
   "source": [
    "%run Setup.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load embedding matrix into an `Embedding` layer. Toggle `trainable=False` to prevent the weights from being updated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM + Deeper CNN Structure\n",
    "[Reference](https://github.com/richliao/textClassifier), [LTSM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/), [CNN Source](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html) and [Notes](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/)\n",
    "\n",
    "Deeper CNN as described in [CNN for Sentence Classification](http://www.aclweb.org/anthology/D14-1181) (Yoon Kim, 2014), multiple filters have been applied. This can be implemented using Keras `Merge` Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "l_lstm1 = Bidirectional(LSTM(12,dropout=0.4,recurrent_dropout=0.4,return_sequences=True))(embedded_sequences)\n",
    "\n",
    "convs, filter_sizes = [], [3,4,5]\n",
    "for fsz in filter_sizes:\n",
    "    l_conv = Conv1D(filters=50,kernel_size=fsz,\n",
    "                    activation='relu',kernel_regularizer=regularizers.l2(0.01))(l_lstm1)\n",
    "    l_pool = MaxPooling1D(2)(l_conv)\n",
    "    convs.append(l_pool)\n",
    "\n",
    "l_merge = Concatenate(axis=1)(convs)\n",
    "l_cov1= Conv1D(50, 3, activation='relu',kernel_regularizer=regularizers.l2(0.02))(l_merge)\n",
    "l_pool1 = MaxPooling1D(2)(l_cov1)\n",
    "l_drop1 = Dropout(0.4)(l_pool1)\n",
    "l_flat = Flatten()(l_drop1)\n",
    "l_dense = Dense(25, activation='relu')(l_flat)\n",
    "\n",
    "preds = Dense(4, activation='softmax')(l_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_metric(optimizer):\n",
    "    def lr(y_true, y_pred):\n",
    "        return optimizer.lr\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Model(sequence_input, preds)\n",
    "adadelta = optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "lr_metric = get_lr_metric(adadelta)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adadelta,\n",
    "              metrics=['acc', lr_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_cyclic(epoch):\n",
    "    try:\n",
    "        l_r, decay = 1.0, 0.0001\n",
    "        if epoch%33==0:multiplier = 10\n",
    "        else:multiplier = 1\n",
    "        rate = float(multiplier * l_r * 1/(1 + decay * epoch))\n",
    "        #print(\"Epoch\",epoch+1,\"- learning_rate\",rate)\n",
    "        return rate\n",
    "    except Exception as e:\n",
    "        print(\"Error in lr_schedule:\",str(e))\n",
    "        return float(1.0)\n",
    "    \n",
    "def initial_boost(epoch):\n",
    "    if epoch==0: return float(6.0)\n",
    "    else: return float(1.0)\n",
    "        \n",
    "tensorboard = callbacks.TensorBoard(log_dir='./logs', histogram_freq=4, batch_size=16, write_grads=True , write_graph=True)\n",
    "model_checkpoints = callbacks.ModelCheckpoint(\"checkpoint.h5\", monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "lr_schedule = callbacks.LearningRateScheduler(step_cyclic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 30, 50)       1534650     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 30, 24)       6048        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 28, 50)       3650        bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 27, 50)       4850        bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 26, 50)       6050        bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 14, 50)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 13, 50)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 13, 50)       0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 40, 50)       0           max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 38, 50)       7550        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 19, 50)       0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 19, 50)       0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 950)          0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 25)           23775       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 4)            104         dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,586,677\n",
      "Trainable params: 1,586,677\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Training Progress:\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/200\n",
      "32000/32000 [==============================] - 36s 1ms/step - loss: 1.2208 - acc: 0.4051 - lr: 10.0000 - val_loss: 1.1222 - val_acc: 0.4989 - val_lr: 10.0000\n",
      "Epoch 2/200\n",
      "32000/32000 [==============================] - 32s 988us/step - loss: 1.1175 - acc: 0.4947 - lr: 0.9999 - val_loss: 1.0889 - val_acc: 0.5216 - val_lr: 0.9999\n",
      "Epoch 3/200\n",
      "32000/32000 [==============================] - 31s 977us/step - loss: 1.0933 - acc: 0.5181 - lr: 0.9998 - val_loss: 1.0703 - val_acc: 0.5340 - val_lr: 0.9998\n",
      "Epoch 4/200\n",
      "32000/32000 [==============================] - 31s 966us/step - loss: 1.0774 - acc: 0.5270 - lr: 0.9997 - val_loss: 1.0521 - val_acc: 0.5465 - val_lr: 0.9997\n",
      "Epoch 5/200\n",
      "32000/32000 [==============================] - 32s 986us/step - loss: 1.0619 - acc: 0.5430 - lr: 0.9996 - val_loss: 1.0460 - val_acc: 0.5487 - val_lr: 0.9996\n",
      "Epoch 6/200\n",
      "32000/32000 [==============================] - 32s 992us/step - loss: 1.0504 - acc: 0.5491 - lr: 0.9995 - val_loss: 1.0366 - val_acc: 0.5555 - val_lr: 0.9995\n",
      "Epoch 7/200\n",
      "32000/32000 [==============================] - 31s 982us/step - loss: 1.0441 - acc: 0.5543 - lr: 0.9994 - val_loss: 1.0300 - val_acc: 0.5579 - val_lr: 0.9994\n",
      "Epoch 8/200\n",
      "32000/32000 [==============================] - 31s 983us/step - loss: 1.0363 - acc: 0.5600 - lr: 0.9993 - val_loss: 1.0271 - val_acc: 0.5651 - val_lr: 0.9993\n",
      "Epoch 9/200\n",
      "32000/32000 [==============================] - 30s 934us/step - loss: 1.0313 - acc: 0.5630 - lr: 0.9992 - val_loss: 1.0211 - val_acc: 0.5673 - val_lr: 0.9992\n",
      "Epoch 10/200\n",
      "32000/32000 [==============================] - 31s 965us/step - loss: 1.0233 - acc: 0.5693 - lr: 0.9991 - val_loss: 1.0204 - val_acc: 0.5680 - val_lr: 0.9991\n",
      "Epoch 11/200\n",
      "32000/32000 [==============================] - 31s 953us/step - loss: 1.0192 - acc: 0.5721 - lr: 0.9990 - val_loss: 1.0182 - val_acc: 0.5664 - val_lr: 0.9990\n",
      "Epoch 12/200\n",
      "32000/32000 [==============================] - 30s 937us/step - loss: 1.0109 - acc: 0.5765 - lr: 0.9989 - val_loss: 1.0100 - val_acc: 0.5706 - val_lr: 0.9989\n",
      "Epoch 13/200\n",
      "32000/32000 [==============================] - 30s 938us/step - loss: 1.0095 - acc: 0.5760 - lr: 0.9988 - val_loss: 1.0072 - val_acc: 0.5750 - val_lr: 0.9988\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "print(\"Training Progress:\")\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          epochs=200, batch_size=64,\n",
    "          callbacks=[tensorboard, model_checkpoints, lr_schedule])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('ltsm-c.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
