{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Text Classification with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Keras version 2.1.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import re, sys, os, csv, keras\n",
    "from many_stop_words import get_stop_words\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras import regularizers, initializers, optimizers, callbacks\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout, LSTM, GRU, Bidirectional\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "print(\"Using Keras version\",keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read from dataset (`csv` file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from csv file...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "texts, labels = [], []\n",
    "print(\"Reading from csv file...\")\n",
    "with open('data.csv') as csvfile:\n",
    "    readCSV = csv.reader(csvfile, delimiter=',')\n",
    "    for row in readCSV:\n",
    "        texts.append(row[0])\n",
    "        labels.append(row[1])\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 40000\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30542 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (40000, 30)\n",
      "Shape of label tensor: (40000, 4)\n"
     ]
    }
   ],
   "source": [
    "labels = to_categorical(np.asarray(labels)) # convert to one-hot encoding vectors\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in each category:\n",
      "Training:\n",
      " [ 7696. 10951. 12182.  1171.]\n",
      "Validation:\n",
      " [1948. 2673. 3117.  262.]\n"
     ]
    }
   ],
   "source": [
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "print('Number of entries in each category:')\n",
    "print(\"Training:\\n\",y_train.sum(axis=0))\n",
    "print(\"Validation:\\n\",y_val.sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Embedding layer\n",
    "Compute an index mapping words to known embeddings, by parsing the data dump of pre-trained embeddings: [GloVe](https://nlp.stanford.edu/projects/glove/) vectors from Stanford NLP. For new words, a \"randomised vector\" will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe from: dataset/glove/glove.twitter.27B.200d.txt ...Done.\n",
      "Proceeding with Embedding Matrix...\n",
      "Completed!\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 200\n",
    "GLOVE_DIR = \"dataset/glove/glove.twitter.27B.200d.txt\"\n",
    "embeddings_index = {}\n",
    "f = open(GLOVE_DIR)\n",
    "print(\"Loading GloVe from:\",GLOVE_DIR,\"...\",end=\"\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print(\"Done.\\nProceeding with Embedding Matrix...\")\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print(\"Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After computing our embedding matrix, load this embedding matrix into an `Embedding` layer. Toggle `trainable=False` to prevent the weights from being updated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified 1D CNN\n",
    "[Reference](https://github.com/richliao/textClassifier), [Source](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html) and [Notes](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32') # 1000\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "l_cov1= Conv1D(128, 3, activation='relu',kernel_regularizer=regularizers.l2(0.01))(embedded_sequences)\n",
    "l_cov2= Conv1D(128, 3, activation='relu',kernel_regularizer=regularizers.l2(0.01))(l_cov1)\n",
    "l_pool1 = MaxPooling1D(3)(l_cov2)\n",
    "#l_drop1 = Dropout(0.2)(l_pool1)\n",
    "l_cov3 = Conv1D(128, 3, activation='relu',kernel_regularizer=regularizers.l2(0.01))(l_pool1)\n",
    "l_pool4 = MaxPooling1D(6)(l_cov3) # global max pooling\n",
    "l_flat = Flatten()(l_pool4)\n",
    "l_dense = Dense(128, activation='relu')(l_flat)\n",
    "preds = Dense(4, activation='softmax')(l_dense) # 4 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 30, 200)           6108600   \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 28, 128)           76928     \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 26, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 8, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 6, 128)            49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 1, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 6,301,116\n",
      "Trainable params: 192,516\n",
      "Non-trainable params: 6,108,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(sequence_input, preds)\n",
    "adadelta = optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adadelta,\n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_cyclic(epoch):\n",
    "    try:\n",
    "        if epoch%11==0:\n",
    "            return float(2)\n",
    "        elif epoch%3==0:\n",
    "            return float(1.1)\n",
    "        elif epoch%7==0:\n",
    "            return float(0.6)\n",
    "        else:\n",
    "            return float(1.0)\n",
    "    except:\n",
    "        return float(1.0)\n",
    "        \n",
    "tensorboard = callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=256, write_grads=True , write_graph=True)\n",
    "model_checkpoints = callbacks.ModelCheckpoint(\"checkpoints\", monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "lr_schedule = callbacks.LearningRateScheduler(step_cyclic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress:\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/200\n",
      "32000/32000 [==============================] - 7s 222us/step - loss: 1.1012 - acc: 0.5102 - val_loss: 1.0972 - val_acc: 0.5131\n",
      "Epoch 2/200\n",
      "32000/32000 [==============================] - 7s 219us/step - loss: 1.0843 - acc: 0.5211 - val_loss: 1.0851 - val_acc: 0.5151\n",
      "Epoch 3/200\n",
      "32000/32000 [==============================] - 7s 228us/step - loss: 1.0840 - acc: 0.5211 - val_loss: 1.0832 - val_acc: 0.5204\n",
      "Epoch 4/200\n",
      "32000/32000 [==============================] - 8s 235us/step - loss: 1.0852 - acc: 0.5214 - val_loss: 1.0934 - val_acc: 0.5072\n",
      "Epoch 5/200\n",
      "32000/32000 [==============================] - 7s 226us/step - loss: 1.0833 - acc: 0.5223 - val_loss: 1.0882 - val_acc: 0.5156\n",
      "Epoch 6/200\n",
      "32000/32000 [==============================] - 7s 231us/step - loss: 1.0834 - acc: 0.5218 - val_loss: 1.0797 - val_acc: 0.5241\n",
      "Epoch 7/200\n",
      "32000/32000 [==============================] - 7s 225us/step - loss: 1.0834 - acc: 0.5215 - val_loss: 1.0895 - val_acc: 0.5162\n",
      "Epoch 8/200\n",
      "32000/32000 [==============================] - 7s 225us/step - loss: 1.0733 - acc: 0.5284 - val_loss: 1.0833 - val_acc: 0.5215\n",
      "Epoch 9/200\n",
      "32000/32000 [==============================] - 7s 227us/step - loss: 1.0817 - acc: 0.5239 - val_loss: 1.1306 - val_acc: 0.4856\n",
      "Epoch 10/200\n",
      "32000/32000 [==============================] - 7s 232us/step - loss: 1.0834 - acc: 0.5198 - val_loss: 1.1206 - val_acc: 0.4946\n",
      "Epoch 11/200\n",
      "32000/32000 [==============================] - 7s 227us/step - loss: 1.0814 - acc: 0.5236 - val_loss: 1.0797 - val_acc: 0.5215\n",
      "Epoch 12/200\n",
      "32000/32000 [==============================] - 7s 230us/step - loss: 1.0995 - acc: 0.5104 - val_loss: 1.0821 - val_acc: 0.5221\n",
      "Epoch 13/200\n",
      "32000/32000 [==============================] - 8s 235us/step - loss: 1.0825 - acc: 0.5228 - val_loss: 1.0803 - val_acc: 0.5212\n",
      "Epoch 14/200\n",
      "32000/32000 [==============================] - 8s 241us/step - loss: 1.0811 - acc: 0.5234 - val_loss: 1.0891 - val_acc: 0.5120\n",
      "Epoch 15/200\n",
      "32000/32000 [==============================] - 8s 238us/step - loss: 1.0733 - acc: 0.5293 - val_loss: 1.0908 - val_acc: 0.5084\n",
      "Epoch 16/200\n",
      "32000/32000 [==============================] - 7s 227us/step - loss: 1.0830 - acc: 0.5217 - val_loss: 1.0836 - val_acc: 0.5172\n",
      "Epoch 17/200\n",
      "32000/32000 [==============================] - 7s 226us/step - loss: 1.0803 - acc: 0.5236 - val_loss: 1.0809 - val_acc: 0.5208\n",
      "Epoch 18/200\n",
      "32000/32000 [==============================] - 7s 227us/step - loss: 1.0803 - acc: 0.5240 - val_loss: 1.0822 - val_acc: 0.5172\n",
      "Epoch 19/200\n",
      "32000/32000 [==============================] - 7s 230us/step - loss: 1.0820 - acc: 0.5203 - val_loss: 1.0831 - val_acc: 0.5179\n",
      "Epoch 20/200\n",
      "32000/32000 [==============================] - 7s 232us/step - loss: 1.0793 - acc: 0.5224 - val_loss: 1.0801 - val_acc: 0.5199\n",
      "Epoch 21/200\n",
      "32000/32000 [==============================] - 7s 230us/step - loss: 1.0786 - acc: 0.5248 - val_loss: 1.0802 - val_acc: 0.5216\n",
      "Epoch 22/200\n",
      "32000/32000 [==============================] - 7s 230us/step - loss: 1.0817 - acc: 0.5212 - val_loss: 1.0984 - val_acc: 0.5106\n",
      "Epoch 23/200\n",
      "32000/32000 [==============================] - 7s 232us/step - loss: 1.0967 - acc: 0.5142 - val_loss: 1.0884 - val_acc: 0.5170\n",
      "Epoch 24/200\n",
      "32000/32000 [==============================] - 8s 236us/step - loss: 1.0805 - acc: 0.5228 - val_loss: 1.0807 - val_acc: 0.5249\n",
      "Epoch 25/200\n",
      "32000/32000 [==============================] - 7s 230us/step - loss: 1.0826 - acc: 0.5200 - val_loss: 1.0927 - val_acc: 0.5080\n",
      "Epoch 26/200\n",
      "32000/32000 [==============================] - 7s 231us/step - loss: 1.0799 - acc: 0.5243 - val_loss: 1.0805 - val_acc: 0.5200\n",
      "Epoch 27/200\n",
      "32000/32000 [==============================] - 7s 232us/step - loss: 1.0783 - acc: 0.5226 - val_loss: 1.0804 - val_acc: 0.5181\n",
      "Epoch 28/200\n",
      "32000/32000 [==============================] - 7s 233us/step - loss: 1.0811 - acc: 0.5212 - val_loss: 1.0804 - val_acc: 0.5186\n",
      "Epoch 29/200\n",
      "32000/32000 [==============================] - 7s 233us/step - loss: 1.0718 - acc: 0.5290 - val_loss: 1.0817 - val_acc: 0.5159\n",
      "Epoch 30/200\n",
      "32000/32000 [==============================] - 7s 234us/step - loss: 1.0790 - acc: 0.5230 - val_loss: 1.0926 - val_acc: 0.5099\n",
      "Epoch 31/200\n",
      "32000/32000 [==============================] - 8s 235us/step - loss: 1.0802 - acc: 0.5222 - val_loss: 1.0832 - val_acc: 0.5182\n",
      "Epoch 32/200\n",
      "32000/32000 [==============================] - 7s 233us/step - loss: 1.0787 - acc: 0.5225 - val_loss: 1.0919 - val_acc: 0.5129\n",
      "Epoch 33/200\n",
      "32000/32000 [==============================] - 7s 233us/step - loss: 1.0778 - acc: 0.5254 - val_loss: 1.0812 - val_acc: 0.5170\n",
      "Epoch 34/200\n",
      "32000/32000 [==============================] - 7s 233us/step - loss: 1.0968 - acc: 0.5120 - val_loss: 1.0860 - val_acc: 0.5106\n",
      "Epoch 35/200\n",
      "32000/32000 [==============================] - 7s 233us/step - loss: 1.0804 - acc: 0.5230 - val_loss: 1.0860 - val_acc: 0.5145\n",
      "Epoch 36/200\n",
      "32000/32000 [==============================] - 7s 232us/step - loss: 1.0730 - acc: 0.5285 - val_loss: 1.0787 - val_acc: 0.5199\n",
      "Epoch 37/200\n",
      "32000/32000 [==============================] - 7s 230us/step - loss: 1.0821 - acc: 0.5199 - val_loss: 1.0800 - val_acc: 0.5198\n",
      "Epoch 38/200\n",
      "32000/32000 [==============================] - 7s 232us/step - loss: 1.0783 - acc: 0.5239 - val_loss: 1.0996 - val_acc: 0.5086\n",
      "Epoch 39/200\n",
      "32000/32000 [==============================] - 7s 232us/step - loss: 1.0787 - acc: 0.5238 - val_loss: 1.0821 - val_acc: 0.5179\n",
      "Epoch 40/200\n",
      "32000/32000 [==============================] - 7s 231us/step - loss: 1.0805 - acc: 0.5223 - val_loss: 1.0799 - val_acc: 0.5200\n",
      "Epoch 41/200\n",
      "32000/32000 [==============================] - 7s 232us/step - loss: 1.0777 - acc: 0.5237 - val_loss: 1.0819 - val_acc: 0.5186\n",
      "Epoch 42/200\n",
      "32000/32000 [==============================] - 7s 231us/step - loss: 1.0779 - acc: 0.5244 - val_loss: 1.0791 - val_acc: 0.5205\n",
      "Epoch 43/200\n",
      "32000/32000 [==============================] - 7s 234us/step - loss: 1.0791 - acc: 0.5234 - val_loss: 1.0816 - val_acc: 0.5188\n",
      "Epoch 44/200\n",
      "32000/32000 [==============================] - 7s 234us/step - loss: 1.0782 - acc: 0.5236 - val_loss: 1.0867 - val_acc: 0.5175\n",
      "Epoch 45/200\n",
      "32000/32000 [==============================] - 7s 234us/step - loss: 1.0944 - acc: 0.5149 - val_loss: 1.0917 - val_acc: 0.5109\n",
      "Epoch 46/200\n",
      "32000/32000 [==============================] - 7s 234us/step - loss: 1.0806 - acc: 0.5236 - val_loss: 1.0778 - val_acc: 0.5200\n",
      "Epoch 47/200\n",
      "32000/32000 [==============================] - 8s 236us/step - loss: 1.0785 - acc: 0.5252 - val_loss: 1.0871 - val_acc: 0.5199\n",
      "Epoch 48/200\n",
      "32000/32000 [==============================] - 7s 234us/step - loss: 1.0798 - acc: 0.5231 - val_loss: 1.0825 - val_acc: 0.5210\n",
      "Epoch 49/200\n",
      "32000/32000 [==============================] - 8s 236us/step - loss: 1.0815 - acc: 0.5219 - val_loss: 1.0828 - val_acc: 0.5139\n",
      "Epoch 50/200\n",
      "32000/32000 [==============================] - 7s 233us/step - loss: 1.0707 - acc: 0.5293 - val_loss: 1.0936 - val_acc: 0.5092\n",
      "Epoch 51/200\n",
      "32000/32000 [==============================] - 8s 234us/step - loss: 1.0782 - acc: 0.5233 - val_loss: 1.0842 - val_acc: 0.5185\n",
      "Epoch 52/200\n",
      "32000/32000 [==============================] - 8s 235us/step - loss: 1.0793 - acc: 0.5222 - val_loss: 1.0798 - val_acc: 0.5200\n",
      "Epoch 53/200\n",
      "32000/32000 [==============================] - 8s 235us/step - loss: 1.0766 - acc: 0.5254 - val_loss: 1.0793 - val_acc: 0.5202\n",
      "Epoch 54/200\n",
      "32000/32000 [==============================] - 8s 242us/step - loss: 1.0783 - acc: 0.5212 - val_loss: 1.0779 - val_acc: 0.5222\n",
      "Epoch 55/200\n",
      "32000/32000 [==============================] - 8s 240us/step - loss: 1.0798 - acc: 0.5219 - val_loss: 1.0781 - val_acc: 0.5201\n",
      "Epoch 56/200\n",
      "32000/32000 [==============================] - 8s 238us/step - loss: 1.0960 - acc: 0.5120 - val_loss: 1.0913 - val_acc: 0.5162\n",
      "Epoch 57/200\n",
      "32000/32000 [==============================] - 8s 236us/step - loss: 1.0728 - acc: 0.5286 - val_loss: 1.0868 - val_acc: 0.5152\n",
      "Epoch 58/200\n",
      "32000/32000 [==============================] - 8s 236us/step - loss: 1.0816 - acc: 0.5224 - val_loss: 1.0892 - val_acc: 0.5118\n",
      "Epoch 59/200\n",
      "32000/32000 [==============================] - 7s 209us/step - loss: 1.0779 - acc: 0.5244 - val_loss: 1.0829 - val_acc: 0.5150\n",
      "Epoch 60/200\n",
      "32000/32000 [==============================] - 7s 209us/step - loss: 1.0776 - acc: 0.5237 - val_loss: 1.0846 - val_acc: 0.5154\n",
      "Epoch 61/200\n",
      "32000/32000 [==============================] - 7s 209us/step - loss: 1.0787 - acc: 0.5235 - val_loss: 1.0825 - val_acc: 0.5201\n",
      "Epoch 62/200\n",
      "32000/32000 [==============================] - 7s 209us/step - loss: 1.0764 - acc: 0.5254 - val_loss: 1.0937 - val_acc: 0.5071\n",
      "Epoch 63/200\n",
      "32000/32000 [==============================] - 7s 227us/step - loss: 1.0783 - acc: 0.5225 - val_loss: 1.0817 - val_acc: 0.5185\n",
      "Epoch 64/200\n",
      "32000/32000 [==============================] - 7s 223us/step - loss: 1.0782 - acc: 0.5223 - val_loss: 1.0848 - val_acc: 0.5119\n",
      "Epoch 65/200\n",
      "32000/32000 [==============================] - 7s 223us/step - loss: 1.0769 - acc: 0.5246 - val_loss: 1.0814 - val_acc: 0.5178\n",
      "Epoch 66/200\n",
      "32000/32000 [==============================] - 7s 224us/step - loss: 1.0769 - acc: 0.5255 - val_loss: 1.0795 - val_acc: 0.5204\n",
      "Epoch 67/200\n",
      "32000/32000 [==============================] - 7s 222us/step - loss: 1.0940 - acc: 0.5143 - val_loss: 1.0835 - val_acc: 0.5191\n",
      "Epoch 68/200\n",
      "32000/32000 [==============================] - 7s 226us/step - loss: 1.0777 - acc: 0.5247 - val_loss: 1.0808 - val_acc: 0.5176\n",
      "Epoch 69/200\n",
      "32000/32000 [==============================] - 7s 222us/step - loss: 1.0783 - acc: 0.5225 - val_loss: 1.0821 - val_acc: 0.5174\n",
      "Epoch 70/200\n",
      "32000/32000 [==============================] - 7s 223us/step - loss: 1.0792 - acc: 0.5251 - val_loss: 1.0817 - val_acc: 0.5176\n",
      "Epoch 71/200\n",
      "32000/32000 [==============================] - 7s 223us/step - loss: 1.0695 - acc: 0.5286 - val_loss: 1.0830 - val_acc: 0.5170\n",
      "Epoch 72/200\n",
      "32000/32000 [==============================] - 7s 223us/step - loss: 1.0770 - acc: 0.5234 - val_loss: 1.0856 - val_acc: 0.5151\n",
      "Epoch 73/200\n",
      "32000/32000 [==============================] - 7s 225us/step - loss: 1.0779 - acc: 0.5223 - val_loss: 1.0838 - val_acc: 0.5201\n",
      "Epoch 74/200\n",
      "32000/32000 [==============================] - 7s 224us/step - loss: 1.0768 - acc: 0.5254 - val_loss: 1.0798 - val_acc: 0.5214\n",
      "Epoch 75/200\n",
      "32000/32000 [==============================] - 7s 223us/step - loss: 1.0755 - acc: 0.5230 - val_loss: 1.0823 - val_acc: 0.5194\n",
      "Epoch 76/200\n",
      "32000/32000 [==============================] - 7s 223us/step - loss: 1.0776 - acc: 0.5251 - val_loss: 1.0817 - val_acc: 0.5184\n",
      "Epoch 77/200\n",
      "32000/32000 [==============================] - 7s 222us/step - loss: 1.0761 - acc: 0.5244 - val_loss: 1.0794 - val_acc: 0.5214\n",
      "Epoch 78/200\n",
      "32000/32000 [==============================] - 7s 223us/step - loss: 1.0949 - acc: 0.5125 - val_loss: 1.1173 - val_acc: 0.4999\n",
      "Epoch 79/200\n",
      "32000/32000 [==============================] - 7s 223us/step - loss: 1.0799 - acc: 0.5236 - val_loss: 1.0840 - val_acc: 0.5142\n",
      "Epoch 80/200\n",
      "32000/32000 [==============================] - 7s 224us/step - loss: 1.0777 - acc: 0.5262 - val_loss: 1.0944 - val_acc: 0.5058\n",
      "Epoch 81/200\n",
      "32000/32000 [==============================] - 7s 225us/step - loss: 1.0777 - acc: 0.5246 - val_loss: 1.0911 - val_acc: 0.5110\n",
      "Epoch 82/200\n",
      "32000/32000 [==============================] - 7s 225us/step - loss: 1.0781 - acc: 0.5245 - val_loss: 1.0841 - val_acc: 0.5135\n",
      "Epoch 83/200\n",
      "32000/32000 [==============================] - 7s 225us/step - loss: 1.0763 - acc: 0.5238 - val_loss: 1.0831 - val_acc: 0.5156\n",
      "Epoch 84/200\n",
      "32000/32000 [==============================] - 7s 224us/step - loss: 1.0765 - acc: 0.5232 - val_loss: 1.0808 - val_acc: 0.5198\n",
      "Epoch 85/200\n",
      "32000/32000 [==============================] - 7s 225us/step - loss: 1.0784 - acc: 0.5228 - val_loss: 1.0976 - val_acc: 0.5036\n",
      "Epoch 86/200\n",
      "32000/32000 [==============================] - 7s 229us/step - loss: 1.0756 - acc: 0.5230 - val_loss: 1.1026 - val_acc: 0.5072\n",
      "Epoch 87/200\n",
      "32000/32000 [==============================] - 7s 211us/step - loss: 1.0761 - acc: 0.5242 - val_loss: 1.0844 - val_acc: 0.5145\n",
      "Epoch 88/200\n",
      "32000/32000 [==============================] - 7s 207us/step - loss: 1.0783 - acc: 0.5242 - val_loss: 1.0839 - val_acc: 0.5188\n",
      "Epoch 89/200\n",
      "32000/32000 [==============================] - 7s 206us/step - loss: 1.0945 - acc: 0.5145 - val_loss: 1.0899 - val_acc: 0.5112\n",
      "Epoch 90/200\n",
      "32000/32000 [==============================] - 7s 224us/step - loss: 1.0771 - acc: 0.5246 - val_loss: 1.0781 - val_acc: 0.5205\n",
      "Epoch 91/200\n",
      "32000/32000 [==============================] - 7s 225us/step - loss: 1.0803 - acc: 0.5240 - val_loss: 1.0812 - val_acc: 0.5162\n",
      "Epoch 92/200\n",
      "32000/32000 [==============================] - 7s 227us/step - loss: 1.0693 - acc: 0.5325 - val_loss: 1.0792 - val_acc: 0.5206\n",
      "Epoch 93/200\n",
      "32000/32000 [==============================] - 7s 225us/step - loss: 1.0762 - acc: 0.5238 - val_loss: 1.0769 - val_acc: 0.5212\n",
      "Epoch 94/200\n",
      "32000/32000 [==============================] - 7s 224us/step - loss: 1.0780 - acc: 0.5234 - val_loss: 1.0816 - val_acc: 0.5166\n",
      "Epoch 95/200\n",
      "32000/32000 [==============================] - 7s 227us/step - loss: 1.0755 - acc: 0.5261 - val_loss: 1.0825 - val_acc: 0.5162\n",
      "Epoch 96/200\n",
      "32000/32000 [==============================] - 8s 235us/step - loss: 1.0767 - acc: 0.5241 - val_loss: 1.0791 - val_acc: 0.5182\n",
      "Epoch 97/200\n",
      "32000/32000 [==============================] - 7s 231us/step - loss: 1.0769 - acc: 0.5254 - val_loss: 1.0798 - val_acc: 0.5165\n",
      "Epoch 98/200\n",
      "32000/32000 [==============================] - 8s 234us/step - loss: 1.0752 - acc: 0.5252 - val_loss: 1.0848 - val_acc: 0.5128\n",
      "Epoch 99/200\n",
      "32000/32000 [==============================] - 7s 226us/step - loss: 1.0678 - acc: 0.5314 - val_loss: 1.0767 - val_acc: 0.5188\n",
      "Epoch 100/200\n",
      "32000/32000 [==============================] - 7s 223us/step - loss: 1.0981 - acc: 0.5111 - val_loss: 1.1165 - val_acc: 0.4920\n",
      "Epoch 101/200\n",
      "32000/32000 [==============================] - 7s 225us/step - loss: 1.0788 - acc: 0.5237 - val_loss: 1.0805 - val_acc: 0.5199\n",
      "Epoch 102/200\n",
      "32000/32000 [==============================] - 7s 227us/step - loss: 1.0759 - acc: 0.5242 - val_loss: 1.0777 - val_acc: 0.5218\n",
      "Epoch 103/200\n",
      "32000/32000 [==============================] - 7s 225us/step - loss: 1.0770 - acc: 0.5235 - val_loss: 1.0846 - val_acc: 0.5125\n",
      "Epoch 104/200\n",
      "32000/32000 [==============================] - 7s 223us/step - loss: 1.0760 - acc: 0.5244 - val_loss: 1.0865 - val_acc: 0.5140\n",
      "Epoch 105/200\n",
      "32000/32000 [==============================] - 7s 223us/step - loss: 1.0770 - acc: 0.5249 - val_loss: 1.0797 - val_acc: 0.5190\n",
      "Epoch 106/200\n",
      "32000/32000 [==============================] - 7s 225us/step - loss: 1.0782 - acc: 0.5219 - val_loss: 1.0856 - val_acc: 0.5135\n",
      "Epoch 107/200\n",
      "32000/32000 [==============================] - 7s 225us/step - loss: 1.0753 - acc: 0.5246 - val_loss: 1.0810 - val_acc: 0.5161\n",
      "Epoch 108/200\n",
      "32000/32000 [==============================] - 7s 224us/step - loss: 1.0760 - acc: 0.5236 - val_loss: 1.0792 - val_acc: 0.5189\n",
      "Epoch 109/200\n",
      "32000/32000 [==============================] - 7s 223us/step - loss: 1.0778 - acc: 0.5232 - val_loss: 1.0858 - val_acc: 0.5149\n",
      "Epoch 110/200\n",
      "32000/32000 [==============================] - 7s 225us/step - loss: 1.0744 - acc: 0.5263 - val_loss: 1.0815 - val_acc: 0.5175\n",
      "Epoch 111/200\n",
      "32000/32000 [==============================] - 7s 225us/step - loss: 1.0920 - acc: 0.5132 - val_loss: 1.1150 - val_acc: 0.5012\n",
      "Epoch 112/200\n",
      "32000/32000 [==============================] - 7s 224us/step - loss: 1.0785 - acc: 0.5240 - val_loss: 1.0819 - val_acc: 0.5174\n",
      "Epoch 113/200\n",
      "32000/32000 [==============================] - 7s 227us/step - loss: 1.0690 - acc: 0.5317 - val_loss: 1.0781 - val_acc: 0.5204\n",
      "Epoch 114/200\n",
      "32000/32000 [==============================] - 7s 227us/step - loss: 1.0753 - acc: 0.5254 - val_loss: 1.0874 - val_acc: 0.5115\n",
      "Epoch 115/200\n",
      "32000/32000 [==============================] - 7s 227us/step - loss: 1.0780 - acc: 0.5246 - val_loss: 1.0849 - val_acc: 0.5169\n",
      "Epoch 116/200\n",
      "32000/32000 [==============================] - 7s 231us/step - loss: 1.0759 - acc: 0.5256 - val_loss: 1.0850 - val_acc: 0.5148\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000/32000 [==============================] - 7s 215us/step - loss: 1.0754 - acc: 0.5270 - val_loss: 1.0779 - val_acc: 0.5231\n",
      "Epoch 118/200\n",
      "32000/32000 [==============================] - 7s 213us/step - loss: 1.0765 - acc: 0.5250 - val_loss: 1.0785 - val_acc: 0.5186\n",
      "Epoch 119/200\n",
      "32000/32000 [==============================] - 7s 213us/step - loss: 1.0761 - acc: 0.5234 - val_loss: 1.0762 - val_acc: 0.5218\n",
      "Epoch 120/200\n",
      "32000/32000 [==============================] - 7s 218us/step - loss: 1.0671 - acc: 0.5312 - val_loss: 1.0740 - val_acc: 0.5251\n",
      "Epoch 121/200\n",
      "32000/32000 [==============================] - 7s 215us/step - loss: 1.0780 - acc: 0.5219 - val_loss: 1.0767 - val_acc: 0.5204\n",
      "Epoch 122/200\n",
      "32000/32000 [==============================] - 7s 216us/step - loss: 1.0933 - acc: 0.5140 - val_loss: 1.0875 - val_acc: 0.5150\n",
      "Epoch 123/200\n",
      "32000/32000 [==============================] - 7s 215us/step - loss: 1.0768 - acc: 0.5256 - val_loss: 1.0783 - val_acc: 0.5198\n",
      "Epoch 124/200\n",
      "32000/32000 [==============================] - 7s 216us/step - loss: 1.0777 - acc: 0.5249 - val_loss: 1.0785 - val_acc: 0.5194\n",
      "Epoch 125/200\n",
      "32000/32000 [==============================] - 7s 218us/step - loss: 1.0766 - acc: 0.5234 - val_loss: 1.0818 - val_acc: 0.5159\n",
      "Epoch 126/200\n",
      "32000/32000 [==============================] - 7s 216us/step - loss: 1.0759 - acc: 0.5244 - val_loss: 1.0801 - val_acc: 0.5210\n",
      "Epoch 127/200\n",
      "32000/32000 [==============================] - 7s 214us/step - loss: 1.0771 - acc: 0.5255 - val_loss: 1.0799 - val_acc: 0.5198\n",
      "Epoch 128/200\n",
      "32000/32000 [==============================] - 7s 217us/step - loss: 1.0759 - acc: 0.5264 - val_loss: 1.0810 - val_acc: 0.5152\n",
      "Epoch 129/200\n",
      "32000/32000 [==============================] - 7s 214us/step - loss: 1.0744 - acc: 0.5274 - val_loss: 1.0806 - val_acc: 0.5196\n",
      "Epoch 130/200\n",
      "32000/32000 [==============================] - 7s 213us/step - loss: 1.0775 - acc: 0.5232 - val_loss: 1.0797 - val_acc: 0.5201\n",
      "Epoch 131/200\n",
      "32000/32000 [==============================] - 7s 218us/step - loss: 1.0735 - acc: 0.5262 - val_loss: 1.0855 - val_acc: 0.5171\n",
      "Epoch 132/200\n",
      "32000/32000 [==============================] - 7s 217us/step - loss: 1.0761 - acc: 0.5238 - val_loss: 1.0821 - val_acc: 0.5164\n",
      "Epoch 133/200\n",
      "32000/32000 [==============================] - 7s 224us/step - loss: 1.0946 - acc: 0.5112 - val_loss: 1.0987 - val_acc: 0.5030\n",
      "Epoch 134/200\n",
      "32000/32000 [==============================] - 7s 216us/step - loss: 1.0720 - acc: 0.5275 - val_loss: 1.0791 - val_acc: 0.5221\n",
      "Epoch 135/200\n",
      "32000/32000 [==============================] - 7s 216us/step - loss: 1.0769 - acc: 0.5217 - val_loss: 1.0813 - val_acc: 0.5179\n",
      "Epoch 136/200\n",
      "32000/32000 [==============================] - 7s 214us/step - loss: 1.0770 - acc: 0.5231 - val_loss: 1.0865 - val_acc: 0.5112\n",
      "Epoch 137/200\n",
      "32000/32000 [==============================] - 7s 215us/step - loss: 1.0746 - acc: 0.5261 - val_loss: 1.0913 - val_acc: 0.5088\n",
      "Epoch 138/200\n",
      "32000/32000 [==============================] - 7s 221us/step - loss: 1.0757 - acc: 0.5251 - val_loss: 1.0812 - val_acc: 0.5165\n",
      "Epoch 139/200\n",
      "32000/32000 [==============================] - 7s 221us/step - loss: 1.0777 - acc: 0.5229 - val_loss: 1.0835 - val_acc: 0.5161\n",
      "Epoch 140/200\n",
      "32000/32000 [==============================] - 7s 220us/step - loss: 1.0740 - acc: 0.5275 - val_loss: 1.0817 - val_acc: 0.5148\n",
      "Epoch 141/200\n",
      "32000/32000 [==============================] - 7s 221us/step - loss: 1.0673 - acc: 0.5304 - val_loss: 1.0853 - val_acc: 0.5146\n",
      "Epoch 142/200\n",
      "32000/32000 [==============================] - 7s 219us/step - loss: 1.0776 - acc: 0.5238 - val_loss: 1.0832 - val_acc: 0.5145\n",
      "Epoch 143/200\n",
      "32000/32000 [==============================] - 7s 218us/step - loss: 1.0749 - acc: 0.5266 - val_loss: 1.0868 - val_acc: 0.5154\n",
      "Epoch 144/200\n",
      "32000/32000 [==============================] - 7s 218us/step - loss: 1.0920 - acc: 0.5114 - val_loss: 1.0967 - val_acc: 0.5061\n",
      "Epoch 145/200\n",
      "32000/32000 [==============================] - 7s 219us/step - loss: 1.0781 - acc: 0.5237 - val_loss: 1.0853 - val_acc: 0.5180\n",
      "Epoch 146/200\n",
      "32000/32000 [==============================] - 7s 218us/step - loss: 1.0741 - acc: 0.5243 - val_loss: 1.0895 - val_acc: 0.5175\n",
      "Epoch 147/200\n",
      "32000/32000 [==============================] - 7s 219us/step - loss: 1.0745 - acc: 0.5264 - val_loss: 1.0792 - val_acc: 0.5195\n",
      "Epoch 148/200\n",
      "32000/32000 [==============================] - 7s 218us/step - loss: 1.0769 - acc: 0.5237 - val_loss: 1.1099 - val_acc: 0.5038\n",
      "Epoch 149/200\n",
      "32000/32000 [==============================] - 7s 219us/step - loss: 1.0769 - acc: 0.5228 - val_loss: 1.0844 - val_acc: 0.5145\n",
      "Epoch 150/200\n",
      "32000/32000 [==============================] - 7s 221us/step - loss: 1.0743 - acc: 0.5241 - val_loss: 1.0956 - val_acc: 0.5104\n",
      "Epoch 151/200\n",
      "32000/32000 [==============================] - 7s 219us/step - loss: 1.0774 - acc: 0.5226 - val_loss: 1.0800 - val_acc: 0.5198\n",
      "Epoch 152/200\n",
      "32000/32000 [==============================] - 7s 220us/step - loss: 1.0761 - acc: 0.5246 - val_loss: 1.0833 - val_acc: 0.5154\n",
      "Epoch 153/200\n",
      "32000/32000 [==============================] - 7s 219us/step - loss: 1.0737 - acc: 0.5250 - val_loss: 1.0835 - val_acc: 0.5140\n",
      "Epoch 154/200\n",
      "32000/32000 [==============================] - 7s 218us/step - loss: 1.0754 - acc: 0.5248 - val_loss: 1.0787 - val_acc: 0.5186\n",
      "Epoch 155/200\n",
      "32000/32000 [==============================] - 7s 220us/step - loss: 1.0933 - acc: 0.5143 - val_loss: 1.0953 - val_acc: 0.5091\n",
      "Epoch 156/200\n",
      "32000/32000 [==============================] - 7s 217us/step - loss: 1.0768 - acc: 0.5253 - val_loss: 1.0774 - val_acc: 0.5215\n",
      "Epoch 157/200\n",
      "32000/32000 [==============================] - 7s 220us/step - loss: 1.0788 - acc: 0.5245 - val_loss: 1.0819 - val_acc: 0.5149\n",
      "Epoch 158/200\n",
      "32000/32000 [==============================] - 7s 219us/step - loss: 1.0740 - acc: 0.5256 - val_loss: 1.0788 - val_acc: 0.5206\n",
      "Epoch 159/200\n",
      "32000/32000 [==============================] - 7s 221us/step - loss: 1.0747 - acc: 0.5241 - val_loss: 1.0905 - val_acc: 0.5082\n",
      "Epoch 160/200\n",
      "32000/32000 [==============================] - 7s 221us/step - loss: 1.0778 - acc: 0.5249 - val_loss: 1.0949 - val_acc: 0.5056\n",
      "Epoch 161/200\n",
      "32000/32000 [==============================] - 7s 223us/step - loss: 1.0738 - acc: 0.5256 - val_loss: 1.0818 - val_acc: 0.5146\n",
      "Epoch 162/200\n",
      "32000/32000 [==============================] - 7s 220us/step - loss: 1.0662 - acc: 0.5317 - val_loss: 1.0855 - val_acc: 0.5118\n",
      "Epoch 163/200\n",
      "32000/32000 [==============================] - 7s 221us/step - loss: 1.0775 - acc: 0.5222 - val_loss: 1.0868 - val_acc: 0.5048\n",
      "Epoch 164/200\n",
      "32000/32000 [==============================] - 7s 222us/step - loss: 1.0755 - acc: 0.5224 - val_loss: 1.0890 - val_acc: 0.5106\n",
      "Epoch 165/200\n",
      "32000/32000 [==============================] - 7s 222us/step - loss: 1.0740 - acc: 0.5245 - val_loss: 1.0815 - val_acc: 0.5138\n",
      "Epoch 166/200\n",
      "32000/32000 [==============================] - 7s 217us/step - loss: 1.0925 - acc: 0.5132 - val_loss: 1.0875 - val_acc: 0.5142\n",
      "Epoch 167/200\n",
      "32000/32000 [==============================] - 7s 221us/step - loss: 1.0760 - acc: 0.5228 - val_loss: 1.0768 - val_acc: 0.5189\n",
      "Epoch 168/200\n",
      "32000/32000 [==============================] - 7s 224us/step - loss: 1.0752 - acc: 0.5247 - val_loss: 1.0778 - val_acc: 0.5179\n",
      "Epoch 169/200\n",
      "32000/32000 [==============================] - 7s 220us/step - loss: 1.0766 - acc: 0.5210 - val_loss: 1.0806 - val_acc: 0.5159\n",
      "Epoch 170/200\n",
      "32000/32000 [==============================] - 7s 222us/step - loss: 1.0758 - acc: 0.5241 - val_loss: 1.0867 - val_acc: 0.5148\n",
      "Epoch 171/200\n",
      "32000/32000 [==============================] - 7s 221us/step - loss: 1.0752 - acc: 0.5247 - val_loss: 1.0793 - val_acc: 0.5162\n",
      "Epoch 172/200\n",
      "32000/32000 [==============================] - 7s 221us/step - loss: 1.0759 - acc: 0.5239 - val_loss: 1.0993 - val_acc: 0.5034\n",
      "Epoch 173/200\n",
      "32000/32000 [==============================] - 7s 222us/step - loss: 1.0731 - acc: 0.5256 - val_loss: 1.0780 - val_acc: 0.5160\n",
      "Epoch 174/200\n",
      "32000/32000 [==============================] - 6s 203us/step - loss: 1.0740 - acc: 0.5266 - val_loss: 1.0804 - val_acc: 0.5155\n",
      "Epoch 175/200\n",
      "32000/32000 [==============================] - 6s 197us/step - loss: 1.0761 - acc: 0.5223 - val_loss: 1.0834 - val_acc: 0.5146\n",
      "Epoch 176/200\n",
      "32000/32000 [==============================] - 6s 195us/step - loss: 1.0668 - acc: 0.5298 - val_loss: 1.0885 - val_acc: 0.5032\n",
      "Epoch 177/200\n",
      "32000/32000 [==============================] - 6s 197us/step - loss: 1.0926 - acc: 0.5122 - val_loss: 1.0801 - val_acc: 0.5178\n",
      "Epoch 178/200\n",
      "32000/32000 [==============================] - 6s 197us/step - loss: 1.0769 - acc: 0.5248 - val_loss: 1.0813 - val_acc: 0.5160\n",
      "Epoch 179/200\n",
      "32000/32000 [==============================] - 6s 196us/step - loss: 1.0762 - acc: 0.5230 - val_loss: 1.0788 - val_acc: 0.5166\n",
      "Epoch 180/200\n",
      "32000/32000 [==============================] - 6s 198us/step - loss: 1.0739 - acc: 0.5277 - val_loss: 1.0802 - val_acc: 0.5154\n",
      "Epoch 181/200\n",
      "32000/32000 [==============================] - 6s 199us/step - loss: 1.0762 - acc: 0.5232 - val_loss: 1.0823 - val_acc: 0.5146\n",
      "Epoch 182/200\n",
      "32000/32000 [==============================] - 7s 203us/step - loss: 1.0749 - acc: 0.5250 - val_loss: 1.0785 - val_acc: 0.5152\n",
      "Epoch 183/200\n",
      "32000/32000 [==============================] - 6s 201us/step - loss: 1.0662 - acc: 0.5315 - val_loss: 1.0869 - val_acc: 0.5104\n",
      "Epoch 184/200\n",
      "32000/32000 [==============================] - 6s 202us/step - loss: 1.0773 - acc: 0.5221 - val_loss: 1.0859 - val_acc: 0.5106\n",
      "Epoch 185/200\n",
      "32000/32000 [==============================] - 6s 197us/step - loss: 1.0743 - acc: 0.5234 - val_loss: 1.0835 - val_acc: 0.5144\n",
      "Epoch 186/200\n",
      "32000/32000 [==============================] - 6s 196us/step - loss: 1.0748 - acc: 0.5252 - val_loss: 1.0890 - val_acc: 0.5096\n",
      "Epoch 187/200\n",
      "32000/32000 [==============================] - 6s 196us/step - loss: 1.0749 - acc: 0.5245 - val_loss: 1.0822 - val_acc: 0.5132\n",
      "Epoch 188/200\n",
      "32000/32000 [==============================] - 6s 197us/step - loss: 1.0924 - acc: 0.5140 - val_loss: 1.0811 - val_acc: 0.5185\n",
      "Epoch 189/200\n",
      "32000/32000 [==============================] - 6s 196us/step - loss: 1.0760 - acc: 0.5227 - val_loss: 1.0838 - val_acc: 0.5151\n",
      "Epoch 190/200\n",
      "32000/32000 [==============================] - 6s 196us/step - loss: 1.0775 - acc: 0.5219 - val_loss: 1.0787 - val_acc: 0.5190\n",
      "Epoch 191/200\n",
      "32000/32000 [==============================] - 6s 199us/step - loss: 1.0735 - acc: 0.5264 - val_loss: 1.0800 - val_acc: 0.5128\n",
      "Epoch 192/200\n",
      "32000/32000 [==============================] - 6s 200us/step - loss: 1.0741 - acc: 0.5237 - val_loss: 1.0766 - val_acc: 0.5191\n",
      "Epoch 193/200\n",
      "32000/32000 [==============================] - 6s 196us/step - loss: 1.0778 - acc: 0.5210 - val_loss: 1.1043 - val_acc: 0.4994\n",
      "Epoch 194/200\n",
      "32000/32000 [==============================] - 6s 197us/step - loss: 1.0741 - acc: 0.5251 - val_loss: 1.0787 - val_acc: 0.5184\n",
      "Epoch 195/200\n",
      "32000/32000 [==============================] - 6s 196us/step - loss: 1.0747 - acc: 0.5235 - val_loss: 1.0934 - val_acc: 0.5048\n",
      "Epoch 196/200\n",
      "32000/32000 [==============================] - 6s 197us/step - loss: 1.0777 - acc: 0.5217 - val_loss: 1.0777 - val_acc: 0.5181\n",
      "Epoch 197/200\n",
      "32000/32000 [==============================] - 6s 202us/step - loss: 1.0662 - acc: 0.5301 - val_loss: 1.0792 - val_acc: 0.5156\n",
      "Epoch 198/200\n",
      "32000/32000 [==============================] - 6s 197us/step - loss: 1.0733 - acc: 0.5251 - val_loss: 1.1110 - val_acc: 0.4969\n",
      "Epoch 199/200\n",
      "32000/32000 [==============================] - 6s 197us/step - loss: 1.0909 - acc: 0.5165 - val_loss: 1.1072 - val_acc: 0.4986\n",
      "Epoch 200/200\n",
      "32000/32000 [==============================] - 6s 195us/step - loss: 1.0758 - acc: 0.5244 - val_loss: 1.0926 - val_acc: 0.5074\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x180516a90>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Training Progress:\")\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          epochs=200, batch_size=128,\n",
    "          callbacks=[tensorboard, model_checkpoints, lr_schedule])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
